# ğŸ® GPUæœºå™¨éƒ¨ç½²æŒ‡å—

## ğŸ“‹ éƒ¨ç½²å‰å‡†å¤‡æ¸…å•

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒæ£€æŸ¥
```bash
# 1. æ£€æŸ¥GPU
nvidia-smi

# 2. æ£€æŸ¥Docker
docker --version

# 3. æ£€æŸ¥Docker Compose
docker compose version

# 4. æ£€æŸ¥NVIDIA Container Runtime
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
```

### ç¬¬äºŒæ­¥ï¼šè·å–ä»£ç 
```bash
# å…‹éš†é¡¹ç›®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
git clone <your-repo-url> translation_app
cd translation_app

# æˆ–è€…æ›´æ–°ç°æœ‰ä»£ç 
git pull origin main
```

### ç¬¬ä¸‰æ­¥ï¼šç¯å¢ƒé…ç½®
```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ–‡ä»¶
cp .env.gpu.example .env

# ç¼–è¾‘ç¯å¢ƒå˜é‡ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
nano .env
```

## ğŸ”§ .env é…ç½®ç¤ºä¾‹ï¼ˆGPUæœºå™¨ï¼‰

```bash
# GPUé…ç½®
CUDA_VISIBLE_DEVICES=0

# æœåŠ¡é…ç½®
OCR_ENGINES=easyocr,paddleocr
MAX_BATCH_SIZE=8
WORKER_TIMEOUT=300

# Ollamaé…ç½®
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODELS=llama3.2:latest,qwen2.5:latest

# æ¨¡å‹é…ç½®
HF_HOME=/app/models/huggingface
MODEL_CACHE_DIR=/app/models

# èµ„æºé™åˆ¶
MAX_MEMORY_OCR=8G
MAX_MEMORY_NMT=12G
MAX_MEMORY_OLLAMA=16G

# è°ƒè¯•é…ç½®
DEBUG=true
LOG_LEVEL=INFO
```

## ğŸš€ åˆ†æ­¥éƒ¨ç½²æµç¨‹

### æ­¥éª¤1ï¼šåŸºç¡€ç¯å¢ƒéªŒè¯
```bash
# è¿è¡Œç¯å¢ƒæ£€æŸ¥è„šæœ¬
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
```

### æ­¥éª¤2ï¼šæ„å»ºæœåŠ¡é•œåƒ
```bash
# æ„å»ºOCRæœåŠ¡
docker compose -f docker-compose.gpu.yml build ocr-service

# æ„å»ºç¿»è¯‘æœåŠ¡  
docker compose -f docker-compose.gpu.yml build nmt-service

# æ„å»ºå®Œæ•´ç¯å¢ƒ
docker compose -f docker-compose.gpu.yml build
```

### æ­¥éª¤3ï¼šä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œé¦–æ¬¡è¿è¡Œï¼‰
```bash
# å¯åŠ¨æ¨¡å‹ä¸‹è½½æœåŠ¡
docker compose -f docker-compose.gpu.yml --profile init up model-downloader

# å¯åŠ¨Ollamaå¹¶ä¸‹è½½æ¨¡å‹
docker compose -f docker-compose.gpu.yml up -d ollama
docker compose -f docker-compose.gpu.yml exec ollama ollama pull llama3.2:latest
```

### æ­¥éª¤4ï¼šå¯åŠ¨GPUæœåŠ¡
```bash
# å¯åŠ¨æ ¸å¿ƒæœåŠ¡ï¼ˆOCR + NMT + Ollamaï¼‰
docker compose -f docker-compose.gpu.yml up -d

# æˆ–å¯åŠ¨å®Œæ•´æœåŠ¡ï¼ˆåŒ…æ‹¬VisionæœåŠ¡ï¼‰
docker compose -f docker-compose.gpu.yml --profile full up -d
```

### æ­¥éª¤5ï¼šéªŒè¯æœåŠ¡
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker compose -f docker-compose.gpu.yml ps

# å¥åº·æ£€æŸ¥
curl http://localhost:7010/health  # OCRæœåŠ¡
curl http://localhost:7020/health  # ç¿»è¯‘æœåŠ¡
curl http://localhost:11434/api/tags  # OllamaæœåŠ¡
```

## ğŸ” æ•…éšœæ’æŸ¥

### GPUç›¸å…³é—®é¢˜
```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# æ£€æŸ¥NVIDIA Dockeræ”¯æŒ
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi

# æ£€æŸ¥å®¹å™¨GPUè®¿é—®
docker compose -f docker-compose.gpu.yml exec ocr-service nvidia-smi
```

### æœåŠ¡å¯åŠ¨é—®é¢˜
```bash
# æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.gpu.yml logs ocr-service
docker compose -f docker-compose.gpu.yml logs nmt-service
docker compose -f docker-compose.gpu.yml logs ollama

# é‡å¯æœåŠ¡
docker compose -f docker-compose.gpu.yml restart ocr-service
```

### æ¨¡å‹ä¸‹è½½é—®é¢˜
```bash
# æ‰‹åŠ¨ä¸‹è½½Ollamaæ¨¡å‹
docker compose -f docker-compose.gpu.yml exec ollama ollama pull llama3.2:latest

# æ£€æŸ¥æ¨¡å‹å­˜å‚¨
docker compose -f docker-compose.gpu.yml exec ollama ollama list
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### å¯åŠ¨ç›‘æ§æœåŠ¡
```bash
# å¯åŠ¨GPUç›‘æ§
docker compose -f docker-compose.gpu.yml --profile monitoring up -d

# è®¿é—®ç›‘æ§é¢æ¿
# GPUç›‘æ§: http://localhost:9400/metrics
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
```bash
# è°ƒæ•´æ‰¹å¤„ç†å¤§å°
export MAX_BATCH_SIZE=16

# è°ƒæ•´å†…å­˜é™åˆ¶
export MAX_MEMORY_OCR=12G
export MAX_MEMORY_NMT=16G
```

## ğŸ¯ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# å¿«é€Ÿå¯åŠ¨
docker compose -f docker-compose.gpu.yml up -d

# æŸ¥çœ‹çŠ¶æ€
docker compose -f docker-compose.gpu.yml ps

# æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.gpu.yml logs -f --tail=50

# é‡å¯æœåŠ¡
docker compose -f docker-compose.gpu.yml restart

# åœæ­¢æœåŠ¡
docker compose -f docker-compose.gpu.yml down

# å®Œå…¨æ¸…ç†
docker compose -f docker-compose.gpu.yml down -v
docker system prune -f
```

## ğŸ”— ä¸å¼€å‘æœºå™¨è”è°ƒ

### å¼€å‘æœºå™¨é…ç½®
åœ¨å¼€å‘æœºå™¨çš„ `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š
```bash
GPU_MACHINE_IP=<GPUæœºå™¨çš„IPåœ°å€>
```

### ç½‘ç»œè¿é€šæ€§æµ‹è¯•
```bash
# åœ¨å¼€å‘æœºå™¨ä¸Šæµ‹è¯•
curl http://<GPUæœºå™¨IP>:7010/health
curl http://<GPUæœºå™¨IP>:7020/health
```