#!/bin/bash

# GPUæœºå™¨ä¸€é”®éƒ¨ç½²è„šæœ¬
# ä½¿ç”¨æ–¹æ³•: ./deploy_gpu.sh [step]

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

# æ£€æŸ¥GPUç¯å¢ƒ
check_gpu() {
    log_step "æ£€æŸ¥GPUç¯å¢ƒ..."
    
    if ! command -v nvidia-smi &> /dev/null; then
        log_error "æœªæ‰¾åˆ°nvidia-smiï¼Œè¯·å®‰è£…NVIDIAé©±åŠ¨"
        exit 1
    fi
    
    log_info "GPUçŠ¶æ€:"
    nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits
    
    # æ£€æŸ¥NVIDIA Dockeræ”¯æŒ
    if ! docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi &> /dev/null; then
        log_error "NVIDIA Dockeræ”¯æŒå¼‚å¸¸ï¼Œè¯·å®‰è£…nvidia-docker2"
        exit 1
    fi
    
    log_info "âœ… GPUç¯å¢ƒæ£€æŸ¥é€šè¿‡"
}

# å‡†å¤‡ç¯å¢ƒ
prepare_env() {
    log_step "å‡†å¤‡ç¯å¢ƒé…ç½®..."
    
    # å¤åˆ¶ç¯å¢ƒå˜é‡æ–‡ä»¶
    if [ ! -f .env ]; then
        cp .env.gpu.example .env
        log_info "å·²åˆ›å»º .env æ–‡ä»¶ï¼Œè¯·æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®"
    else
        log_info ".env æ–‡ä»¶å·²å­˜åœ¨"
    fi
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    mkdir -p models/{ocr,nmt,vision,huggingface,diffusers}
    mkdir -p temp/{uploads,processed}
    mkdir -p logs
    
    log_info "âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ"
}

# æ„å»ºé•œåƒ
build_images() {
    log_step "æ„å»ºDockeré•œåƒ..."
    
    log_info "æ„å»ºOCRæœåŠ¡é•œåƒ..."
    docker compose -f docker-compose.gpu.yml build ocr-service
    
    log_info "æ„å»ºç¿»è¯‘æœåŠ¡é•œåƒ..."
    docker compose -f docker-compose.gpu.yml build nmt-service
    
    # å¦‚æœå­˜åœ¨VisionæœåŠ¡ï¼Œä¹Ÿæ„å»º
    if docker compose -f docker-compose.gpu.yml config --services | grep -q vision-service; then
        log_info "æ„å»ºVisionæœåŠ¡é•œåƒ..."
        docker compose -f docker-compose.gpu.yml build vision-service
    fi
    
    log_info "âœ… é•œåƒæ„å»ºå®Œæˆ"
}

# ä¸‹è½½æ¨¡å‹
download_models() {
    log_step "ä¸‹è½½AIæ¨¡å‹..."
    
    # å¯åŠ¨Ollama
    log_info "å¯åŠ¨OllamaæœåŠ¡..."
    docker compose -f docker-compose.gpu.yml up -d ollama
    
    # ç­‰å¾…Ollamaå¯åŠ¨
    log_info "ç­‰å¾…OllamaæœåŠ¡å°±ç»ª..."
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/tags &> /dev/null; then
            break
        fi
        echo -n "."
        sleep 2
    done
    echo ""
    
    # ä¸‹è½½æ¨¡å‹
    log_info "ä¸‹è½½Llamaæ¨¡å‹..."
    docker compose -f docker-compose.gpu.yml exec ollama ollama pull llama3.2:latest
    
    log_info "ä¸‹è½½Qwenæ¨¡å‹..."
    docker compose -f docker-compose.gpu.yml exec ollama ollama pull qwen2.5:latest || log_warn "Qwenæ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå¯è·³è¿‡"
    
    log_info "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ"
}

# å¯åŠ¨æœåŠ¡
start_services() {
    log_step "å¯åŠ¨GPUæœåŠ¡..."
    
    # å¯åŠ¨æ ¸å¿ƒæœåŠ¡
    log_info "å¯åŠ¨æ ¸å¿ƒæœåŠ¡..."
    docker compose -f docker-compose.gpu.yml up -d ocr-service nmt-service ollama
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    log_info "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
    sleep 30
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    log_info "æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
    docker compose -f docker-compose.gpu.yml ps
    
    log_info "âœ… æœåŠ¡å¯åŠ¨å®Œæˆ"
}

# å¥åº·æ£€æŸ¥
health_check() {
    log_step "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    
    services=("http://localhost:7010/health" "http://localhost:7020/health" "http://localhost:11434/api/tags")
    names=("OCRæœåŠ¡" "ç¿»è¯‘æœåŠ¡" "OllamaæœåŠ¡")
    
    for i in "${!services[@]}"; do
        url="${services[$i]}"
        name="${names[$i]}"
        
        if curl -f -s "$url" &> /dev/null; then
            log_info "âœ… $name å¥åº·æ£€æŸ¥é€šè¿‡"
        else
            log_error "âŒ $name å¥åº·æ£€æŸ¥å¤±è´¥"
            log_info "å°è¯•æŸ¥çœ‹æ—¥å¿—: docker compose -f docker-compose.gpu.yml logs ${url##*/}"
        fi
    done
}

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
show_help() {
    echo "GPUæœºå™¨éƒ¨ç½²è„šæœ¬ä½¿ç”¨è¯´æ˜:"
    echo ""
    echo "  ./deploy_gpu.sh [æ­¥éª¤]"
    echo ""
    echo "å¯ç”¨æ­¥éª¤:"
    echo "  check      - æ£€æŸ¥GPUç¯å¢ƒ"
    echo "  prepare    - å‡†å¤‡ç¯å¢ƒé…ç½®" 
    echo "  build      - æ„å»ºDockeré•œåƒ"
    echo "  models     - ä¸‹è½½AIæ¨¡å‹"
    echo "  start      - å¯åŠ¨æœåŠ¡"
    echo "  health     - å¥åº·æ£€æŸ¥"
    echo "  all        - æ‰§è¡Œå®Œæ•´éƒ¨ç½²æµç¨‹"
    echo "  logs       - æŸ¥çœ‹æœåŠ¡æ—¥å¿—"
    echo "  status     - æŸ¥çœ‹æœåŠ¡çŠ¶æ€"
    echo "  stop       - åœæ­¢æœåŠ¡"
    echo "  clean      - æ¸…ç†ç¯å¢ƒ"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  ./deploy_gpu.sh all       # å®Œæ•´éƒ¨ç½²"
    echo "  ./deploy_gpu.sh check     # åªæ£€æŸ¥ç¯å¢ƒ"
    echo "  ./deploy_gpu.sh start     # åªå¯åŠ¨æœåŠ¡"
}

# æŸ¥çœ‹æ—¥å¿—
show_logs() {
    log_info "æ˜¾ç¤ºæœåŠ¡æ—¥å¿—..."
    docker compose -f docker-compose.gpu.yml logs -f --tail=50
}

# æŸ¥çœ‹çŠ¶æ€
show_status() {
    log_info "æœåŠ¡çŠ¶æ€:"
    docker compose -f docker-compose.gpu.yml ps
    echo ""
    log_info "GPUä½¿ç”¨æƒ…å†µ:"
    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits
}

# åœæ­¢æœåŠ¡
stop_services() {
    log_info "åœæ­¢GPUæœåŠ¡..."
    docker compose -f docker-compose.gpu.yml down
    log_info "âœ… æœåŠ¡å·²åœæ­¢"
}

# æ¸…ç†ç¯å¢ƒ
clean_env() {
    log_warn "è¿™å°†åˆ é™¤æ‰€æœ‰å®¹å™¨å’Œæ•°æ®ï¼Œç¡®å®šè¦ç»§ç»­å—? (y/N)"
    read -r response
    if [[ "$response" =~ ^([yY][eE][sS]|[yY])+$ ]]; then
        log_info "æ¸…ç†Dockerç¯å¢ƒ..."
        docker compose -f docker-compose.gpu.yml down -v
        docker system prune -f
        log_info "âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ"
    else
        log_info "å–æ¶ˆæ¸…ç†æ“ä½œ"
    fi
}

# ä¸»å‡½æ•°
main() {
    case "${1:-help}" in
        "check")
            check_gpu
            ;;
        "prepare")
            prepare_env
            ;;
        "build")
            build_images
            ;;
        "models")
            download_models
            ;;
        "start")
            start_services
            ;;
        "health")
            health_check
            ;;
        "all")
            log_info "ğŸš€ å¼€å§‹GPUæœºå™¨å®Œæ•´éƒ¨ç½²æµç¨‹..."
            check_gpu
            prepare_env
            build_images
            download_models
            start_services
            health_check
            log_info "ğŸ‰ GPUæœºå™¨éƒ¨ç½²å®Œæˆ!"
            log_info "ğŸ“Š æœåŠ¡åœ°å€:"
            log_info "  OCRæœåŠ¡: http://localhost:7010/docs"
            log_info "  ç¿»è¯‘æœåŠ¡: http://localhost:7020/docs"  
            log_info "  Ollama: http://localhost:11434"
            ;;
        "logs")
            show_logs
            ;;
        "status")
            show_status
            ;;
        "stop")
            stop_services
            ;;
        "clean")
            clean_env
            ;;
        "help"|*)
            show_help
            ;;
    esac
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"