# GPUæœºå™¨éƒ¨ç½²æŒ‡å—

## ğŸ¯ GPUæœºå™¨éœ€è¦éƒ¨ç½²çš„æœåŠ¡

### 1. OCRæœåŠ¡ (Port: 7010)
```bash
# ä¾èµ–å®‰è£…
pip install easyocr paddleocr torch torchvision
pip install transformers opencv-python pillow

# æœåŠ¡ç‰¹ç‚¹
- æ”¯æŒä¸­è‹±æ–‡OCR
- GPUåŠ é€Ÿæ¨ç†
- å¤šå¼•æ“é™çº§æœºåˆ¶
- å†…å­˜éœ€æ±‚: 4-8GB GPU
```

### 2. ç¿»è¯‘æœåŠ¡ (Port: 7020)
```bash
# Ollamaå®‰è£…
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3.2:latest
ollama pull qwen2.5:latest

# CTranslate2å®‰è£…
pip install ctranslate2 transformers sentencepiece

# æœåŠ¡ç‰¹ç‚¹
- æ”¯æŒå¤šç§ç¿»è¯‘å¼•æ“
- æ‰¹é‡ç¿»è¯‘ä¼˜åŒ–
- å†…å­˜éœ€æ±‚: 8-16GB GPU
```

### 3. å›¾åƒå¤„ç†æœåŠ¡ (Port: 7030)
```bash
# Stable Diffusionä¾èµ–
pip install diffusers accelerate xformers
pip install controlnet-aux

# æœåŠ¡ç‰¹ç‚¹
- å›¾åƒä¿®å¤å’Œé‡ç»˜
- æ–‡å­—åŒºåŸŸæ™ºèƒ½å¡«å……
- å†…å­˜éœ€æ±‚: 8-12GB GPU
```

## ğŸ”§ Nginxé…ç½® (å¼€å‘æœºå™¨)

### /etc/nginx/sites-available/document-translator
```nginx
# æ–‡æ¡£ç¿»è¯‘æœåŠ¡ Nginxé…ç½®
upstream orchestrator_backend {
    server localhost:8000;
}

upstream file_service_backend {
    server localhost:8010;
}

# GPUæœºå™¨çš„æ¨¡å‹æœåŠ¡ (æ›¿æ¢ä¸ºå®é™…GPUæœºå™¨IP)
upstream ocr_service_backend {
    server 192.168.1.100:7010;  # GPUæœºå™¨IP
}

upstream nmt_service_backend {
    server 192.168.1.100:7020;  # GPUæœºå™¨IP
}

upstream vision_service_backend {
    server 192.168.1.100:7030;  # GPUæœºå™¨IP
}

server {
    listen 80;
    server_name localhost document-translator.local;
    
    # å‰ç«¯é™æ€æ–‡ä»¶
    location / {
        root /var/www/document-translator/frontend/dist;
        try_files $uri $uri/ /index.html;
        add_header Cache-Control "no-cache, no-store, must-revalidate";
    }
    
    # APIç½‘å…³ - ç¼–æ’æœåŠ¡
    location /api/ {
        proxy_pass http://orchestrator_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # é•¿æ—¶é—´è¯·æ±‚æ”¯æŒ
        proxy_read_timeout 300s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 120s;
        
        # å¤§æ–‡ä»¶ä¸Šä¼ 
        client_max_body_size 100M;
    }
    
    # æ–‡ä»¶æœåŠ¡
    location /files/ {
        proxy_pass http://file_service_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # å¤§æ–‡ä»¶ä¸Šä¼ 
        client_max_body_size 100M;
    }
    
    # OCRæœåŠ¡ä»£ç† (åˆ°GPUæœºå™¨)
    location /ocr/ {
        proxy_pass http://ocr_service_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # OCRå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        proxy_read_timeout 120s;
        proxy_connect_timeout 30s;
    }
    
    # ç¿»è¯‘æœåŠ¡ä»£ç† (åˆ°GPUæœºå™¨)
    location /translate/ {
        proxy_pass http://nmt_service_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # ç¿»è¯‘å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        proxy_read_timeout 180s;
        proxy_connect_timeout 30s;
    }
    
    # å›¾åƒå¤„ç†æœåŠ¡ä»£ç† (åˆ°GPUæœºå™¨)
    location /vision/ {
        proxy_pass http://vision_service_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # å›¾åƒå¤„ç†éœ€è¦å¾ˆé•¿æ—¶é—´
        proxy_read_timeout 300s;
        proxy_connect_timeout 30s;
    }
    
    # å¥åº·æ£€æŸ¥
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
    
    # WebSocketæ”¯æŒ (å¦‚æœéœ€è¦å®æ—¶æ›´æ–°)
    location /ws/ {
        proxy_pass http://orchestrator_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}

# HTTPSé…ç½® (ç”Ÿäº§ç¯å¢ƒ)
server {
    listen 443 ssl http2;
    server_name document-translator.yourdomain.com;
    
    ssl_certificate /etc/ssl/certs/document-translator.crt;
    ssl_certificate_key /etc/ssl/private/document-translator.key;
    
    # SSLå®‰å…¨é…ç½®
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;
    
    # é‡å®šå‘åˆ°ä¸Šé¢çš„é…ç½®
    location / {
        proxy_pass http://localhost:80;
    }
}
```

## ğŸ³ Docker Compose (GPUæœºå™¨)

### gpu-services/docker-compose.yml
```yaml
version: '3.8'

services:
  ocr-service:
    build: ./ocr-service
    ports:
      - "7010:7010"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
    volumes:
      - ./models:/app/models
      - ./temp:/app/temp
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7010/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nmt-service:
    build: ./nmt-service
    ports:
      - "7020:7020"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_HOST=http://localhost:11434
    volumes:
      - ./models:/app/models
      - ./temp:/app/temp
      - ollama_data:/root/.ollama
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  vision-service:
    build: ./vision-service
    ports:
      - "7030:7030"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/app/models/huggingface
    volumes:
      - ./models:/app/models
      - ./temp:/app/temp
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  ollama_data:
```

## ğŸ”„ éƒ¨ç½²æµç¨‹

### ç¬¬ä¸€é˜¶æ®µ: å¼€å‘æœºå™¨æ¡†æ¶æ­å»º
1. å®Œå–„OrchestratoræœåŠ¡
2. å¼€å‘File Service
3. æ­å»ºå‰ç«¯ç•Œé¢
4. é…ç½®Nginxåå‘ä»£ç†
5. è®¾ç½®PostgreSQL + Redis

### ç¬¬äºŒé˜¶æ®µ: GPUæœºå™¨æ¨¡å‹éƒ¨ç½²
1. éƒ¨ç½²OCRæœåŠ¡ (EasyOCR + PaddleOCR)
2. éƒ¨ç½²ç¿»è¯‘æœåŠ¡ (Ollama + CTranslate2)
3. éƒ¨ç½²å›¾åƒå¤„ç†æœåŠ¡
4. é…ç½®æœåŠ¡å‘ç°å’Œå¥åº·æ£€æŸ¥

### ç¬¬ä¸‰é˜¶æ®µ: è”è°ƒæµ‹è¯•
1. ç½‘ç»œè¿é€šæ€§æµ‹è¯•
2. ç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•
3. æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯•
4. ç›‘æ§å’Œæ—¥å¿—é…ç½®